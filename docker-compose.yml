services:
  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    environment:
      MONGO_URI: "mongodb://mongodb:27017"
      CELERY_BROKER_URL: "amqp://guest:guest@rabbitmq:5672//"

  backend:
    build: ./backend
    ports:
      - "8080:8080"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
      - RABBITMQ_DEFAULT_VHOST=/
      - CELERY_BROKER_URL=amqp://admin:admin@rabbitmq:5672/
      - CELERY_RESULT_BACKEND=rpc://
      - CELERY_LOG_LEVEL=debug
      - FLOWER_BASIC_AUTH=admin:password
    depends_on:
      - mysql
      - mongodb
      - rabbitmq
      - flower
    command: celery -A app.celery_task.tasks worker --loglevel=info

  flower:
    build: ./backend
    environment:
      - CELERY_BROKER_URL=amqp://admin:admin@rabbitmq:5672//
    ports:
      - "5555:5555"
    depends_on:
      - rabbitmq
    command: flower --app=app.celery_task.tasks --broker=amqp://admin:admin@rabbitmq:5672// --port=5555

  mysql:
    build: ./mysql
    command: --default-authentication-plugin=mysql_native_password
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: Zm.1575098153

  mongodb:
    build: ./mongodb
    ports:
      - "27017:27017"
    volumes:
      - ./mongodb/data:/data/db

  elasticsearch:
    build: ./elasticsearch
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"
      - "9300:9300"

  rabbitmq:
    build: ./rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"



  redis:
    build: ./redis
    ports:
      - "6379:6379"

  spacy:
    build: ./spacy



  master:
    image: my-spark-image
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master --ip spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "50070:50070"
    volumes:
      - ./data:/data
      - ./logs:/opt/hadoop/logs
      - ./spark-logs:/opt/spark/logs
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=spark-master
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_HOME=/opt/spark
    depends_on:
      - worker1
      - worker2

  worker1:
    image: my-spark-image
    container_name: spark-worker1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker --ip spark-worker1 --webui-port 8081 spark://spark-master:7077
    volumes:
      - ./data:/data
      - ./logs:/opt/hadoop/logs
      - ./spark-logs:/opt/spark/logs
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=spark-worker1
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_HOME=/opt/spark

  worker2:
    image: my-spark-image
    container_name: spark-worker2
    command: bin/spark-class org.apache.spark.deploy.worker.Worker --ip spark-worker2 --webui-port 8081 spark://spark-master:7077
    volumes:
      - ./data:/data
      - ./logs:/opt/hadoop/logs
      - ./spark-logs:/opt/spark/logs
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_LOCAL_IP=spark-worker2
      - HADOOP_HOME=/opt/hadoop
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_HOME=/opt/spark

networks:
  es-net:
    driver: bridge